<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>CS 180 Project 3: Image Mosaicing</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: 'Georgia', serif;
            line-height: 1.6;
            color: #333;
            max-width: 1200px;
            margin: 0 auto;
            padding: 40px 20px;
            background-color: #fefefe;
        }
        
        .header {
            text-align: center;
            margin-bottom: 50px;
            border-bottom: 2px solid #2c3e50;
            padding-bottom: 30px;
        }
        
        h1 {
            font-size: 2.2em;
            color: #2c3e50;
            margin-bottom: 10px;
            font-weight: normal;
        }
        
        .author {
            font-size: 1.2em;
            color: #7f8c8d;
            margin-bottom: 5px;
        }
        
        .intro {
            font-size: 1.1em;
            margin-bottom: 40px;
            text-align: justify;
        }
        
        h2 {
            font-size: 1.8em;
            color: #2c3e50;
            margin: 50px 0 20px 0;
            border-bottom: 1px solid #bdc3c7;
            padding-bottom: 10px;
        }
        
        h3 {
            font-size: 1.4em;
            color: #34495e;
            margin: 30px 0 15px 0;
        }
        
        h4 {
            font-size: 1.2em;
            color: #34495e;
            margin: 25px 0 15px 0;
        }
        
        p {
            margin-bottom: 20px;
            text-align: justify;
        }
        
        .image-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 20px;
            margin: 30px 0;
        }
        
        .image-pair {
            text-align: center;
        }
        
        .image-pair img {
            max-width: 100%;
            height: auto;
            border: 1px solid #ddd;
            box-shadow: 0 2px 5px rgba(0,0,0,0.1);
        }
        
        .image-caption {
            font-style: italic;
            color: #7f8c8d;
            margin-top: 8px;
            font-size: 0.9em;
        }
        
        .matrix {
            background-color: #f8f9fa;
            padding: 20px;
            border-radius: 5px;
            margin: 20px 0;
            overflow-x: auto;
            font-family: 'Courier New', monospace;
            border-left: 4px solid #3498db;
            font-size: 0.85em;
            line-height: 1.8;
        }
        
        .equation {
            text-align: center;
            margin: 25px 0;
            font-family: 'Cambria', serif;
        }
        
        .method-comparison {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 30px;
            margin: 30px 0;
        }
        
        .method {
            padding: 20px;
            background-color: #f8f9fa;
            border-radius: 5px;
        }
        
        .procedure {
            background-color: #f8f9fa;
            padding: 25px;
            border-radius: 5px;
            margin: 30px 0;
            border-left: 4px solid #27ae60;
        }
        
        .procedure-step {
            margin-bottom: 15px;
            padding-left: 20px;
        }
        
        .procedure-step:last-child {
            margin-bottom: 0;
        }
        
        .point-table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
        }
        
        .point-table th, .point-table td {
            border: 1px solid #ddd;
            padding: 12px;
            text-align: center;
        }
        
        .point-table th {
            background-color: #f2f2f2;
            font-weight: bold;
        }
        
        ul {
            margin: 20px 0;
            padding-left: 40px;
        }
        
        li {
            margin-bottom: 10px;
        }
        
        .highlight-box {
            background-color: #fff3cd;
            border-left: 4px solid #ffc107;
            padding: 20px;
            margin: 20px 0;
            border-radius: 5px;
        }
        
        @media (max-width: 768px) {
            .method-comparison {
                grid-template-columns: 1fr;
            }
            
            .image-grid {
                grid-template-columns: 1fr;
            }
        }
    </style>
</head>
<body>
    <div class="header">
        <h1>CS 180 Project 3: Image Mosaicing</h1>
        <div class="author">Jessie</div>
    </div>

    <div class="intro">
        <p>This project implements a complete image mosaicing pipeline from capturing photographs to creating seamless panoramic composites. The implementation includes homography computation, image warping with two interpolation methods, and sophisticated blending techniques to merge multiple images into cohesive mosaics.</p>
    </div>

    <h2>Part A: Image Warping and Mosaicing</h2>

    <h3>A.1: Images used in this project</h3>
    <p>Images were captured with a fixed center of projection while rotating the camera to ensure projective transformations between frames. Each pair maintains 40-70% overlap as recommended, providing sufficient common features for accurate homography computation.</p>

    <div class="image-grid">
        <div class="image-pair">
            <img src="image1.jpg" alt="Image 1">
            <div class="image-caption">Image 1 (5712×4284)</div>
        </div>
        <div class="image-pair">
            <img src="image2.jpg" alt="Image 2">
            <div class="image-caption">Image 2 (5712×4284)</div>
        </div>
        <div class="image-pair">
            <img src="image3.jpg" alt="Image 3">
            <div class="image-caption">Image 3 (5712×4284)</div>
        </div>
        <div class="image-pair">
            <img src="image4.jpg" alt="Image 4">
            <div class="image-caption">Image 4 (5712×4284)</div>
        </div>
        <div class="image-pair">
            <img src="image5.jpg" alt="Image 5">
            <div class="image-caption">Image 5 (5712×4284)</div>
        </div>
        <div class="image-pair">
            <img src="image6.jpg" alt="Image 6">
            <div class="image-caption">Image 6 (5712×4284)</div>
        </div>
    </div>

    <h3>A.2: Homography Recovery</h3>
    <p>The homography matrix H transforms points from one image plane to another, accounting for perspective changes. For this project, I implemented the computeH function that solves for the 3×3 homography matrix using point correspondences.</p>

    <h4>Point Correspondences - First Image Pair</h4>
    <p>For the first pair of pictures, six point pairs were manually selected between the reference and target images:</p>
    <div class="image-pair">
        <img src="akk.png" alt="First image pair with point correspondences">
        <div class="image-caption">First Image Pair - Point Correspondences Visualization</div>
    </div>

    <table class="point-table">
        <tr>
            <th>Point #</th>
            <th>Reference Image (x, y)</th>
            <th>Target Image (x', y')</th>
        </tr>
        <tr>
            <td>1</td>
            <td>(5178.32, 2214.52)</td>
            <td>(2347.82, 2304.93)</td>
        </tr>
        <tr>
            <td>2</td>
            <td>(5609.50, 2169.32)</td>
            <td>(2646.86, 2284.07)</td>
        </tr>
        <tr>
            <td>3</td>
            <td>(4938.39, 778.41)</td>
            <td>(2187.86, 990.52)</td>
        </tr>
        <tr>
            <td>4</td>
            <td>(5491.27, 562.82)</td>
            <td>(2619.05, 910.55)</td>
        </tr>
        <tr>
            <td>5</td>
            <td>(4103.84, 2367.52)</td>
            <td>(1346.36, 2430.11)</td>
        </tr>
        <tr>
            <td>6</td>
            <td>(4117.75, 1098.32)</td>
            <td>(1440.25, 1133.09)</td>
        </tr>
    </table>

    <h4>Homography Computation</h4>
    <p>Each point correspondence generates two equations in the homography parameters:</p>

    <div class="equation">
        x·h<sub>11</sub> + y·h<sub>12</sub> + h<sub>13</sub> - x'·x·h<sub>31</sub> - x'·y·h<sub>32</sub> - x'·h<sub>33</sub> = 0<br>
        x·h<sub>21</sub> + y·h<sub>22</sub> + h<sub>23</sub> - y'·x·h<sub>31</sub> - y'·y·h<sub>32</sub> - y'·h<sub>33</sub> = 0
    </div>

    <p>With 6 point pairs, we have 12 equations for 9 unknowns, forming an overdetermined system solved using Singular Value Decomposition (SVD) for optimal least-squares solution.</p>

    <h4>System Matrix A</h4>
    <p>The homography computation requires solving the linear system Ah = 0, where A is constructed from the point correspondences. Each point pair (x, y) → (x', y') contributes two rows to the matrix:</p>
    
    <div class="matrix">
A = [<br>
&nbsp;&nbsp;[&nbsp;5178.31818,&nbsp;&nbsp;2214.52273,&nbsp;&nbsp;1.00000,&nbsp;&nbsp;0.00000,&nbsp;&nbsp;0.00000,&nbsp;&nbsp;0.00000,&nbsp;-12157749.6,&nbsp;-5199296.72,&nbsp;-2347.81818],<br>
&nbsp;&nbsp;[&nbsp;0.00000,&nbsp;&nbsp;0.00000,&nbsp;&nbsp;0.00000,&nbsp;&nbsp;5178.31818,&nbsp;&nbsp;2214.52273,&nbsp;&nbsp;1.00000,&nbsp;-11935670.3,&nbsp;-5104323.90,&nbsp;-2304.93182],<br>
&nbsp;&nbsp;[&nbsp;5609.50000,&nbsp;&nbsp;2169.31818,&nbsp;&nbsp;1.00000,&nbsp;&nbsp;0.00000,&nbsp;&nbsp;0.00000,&nbsp;&nbsp;0.00000,&nbsp;-14847581.6,&nbsp;-5741889.41,&nbsp;-2646.86364],<br>
&nbsp;&nbsp;[&nbsp;0.00000,&nbsp;&nbsp;0.00000,&nbsp;&nbsp;0.00000,&nbsp;&nbsp;5609.50000,&nbsp;&nbsp;2169.31818,&nbsp;&nbsp;1.00000,&nbsp;-12812480.5,&nbsp;-4954870.64,&nbsp;-2284.06818],<br>
&nbsp;&nbsp;[&nbsp;4938.38636,&nbsp;&nbsp;778.40909,&nbsp;&nbsp;1.00000,&nbsp;&nbsp;0.00000,&nbsp;&nbsp;0.00000,&nbsp;&nbsp;0.00000,&nbsp;-10804515.9,&nbsp;-1703052.94,&nbsp;-2187.86364],<br>
&nbsp;&nbsp;[&nbsp;0.00000,&nbsp;&nbsp;0.00000,&nbsp;&nbsp;0.00000,&nbsp;&nbsp;4938.38636,&nbsp;&nbsp;778.40909,&nbsp;&nbsp;1.00000,&nbsp;-4891583.93,&nbsp;-771031.896,&nbsp;-990.522727],<br>
&nbsp;&nbsp;[&nbsp;5491.27273,&nbsp;&nbsp;562.81818,&nbsp;&nbsp;1.00000,&nbsp;&nbsp;0.00000,&nbsp;&nbsp;0.00000,&nbsp;&nbsp;0.00000,&nbsp;-14381892.9,&nbsp;-1474046.40,&nbsp;-2619.04545],<br>
&nbsp;&nbsp;[&nbsp;0.00000,&nbsp;&nbsp;0.00000,&nbsp;&nbsp;0.00000,&nbsp;&nbsp;5491.27273,&nbsp;&nbsp;562.81818,&nbsp;&nbsp;1.00000,&nbsp;-5000053.42,&nbsp;-512471.537,&nbsp;-910.545455],<br>
&nbsp;&nbsp;[&nbsp;4103.84091,&nbsp;&nbsp;2367.52273,&nbsp;&nbsp;1.00000,&nbsp;&nbsp;0.00000,&nbsp;&nbsp;0.00000,&nbsp;&nbsp;0.00000,&nbsp;-5525262.17,&nbsp;-3187546.51,&nbsp;-1346.36364],<br>
&nbsp;&nbsp;[&nbsp;0.00000,&nbsp;&nbsp;0.00000,&nbsp;&nbsp;0.00000,&nbsp;&nbsp;4103.84091,&nbsp;&nbsp;2367.52273,&nbsp;&nbsp;1.00000,&nbsp;-9972799.75,&nbsp;-5753349.26,&nbsp;-2430.11364],<br>
&nbsp;&nbsp;[&nbsp;4117.75000,&nbsp;&nbsp;1098.31818,&nbsp;&nbsp;1.00000,&nbsp;&nbsp;0.00000,&nbsp;&nbsp;0.00000,&nbsp;&nbsp;0.00000,&nbsp;-5930589.44,&nbsp;-1581852.76,&nbsp;-1440.25000],<br>
&nbsp;&nbsp;[&nbsp;0.00000,&nbsp;&nbsp;0.00000,&nbsp;&nbsp;0.00000,&nbsp;&nbsp;4117.75000,&nbsp;&nbsp;1098.31818,&nbsp;&nbsp;1.00000,&nbsp;-4665785.09,&nbsp;-1244494.35,&nbsp;-1133.09091]<br>
]
    </div>
    
    <p>This 12×9 matrix represents the overdetermined system derived from 6 point correspondences. The homography parameters are found by computing the null space of A using Singular Value Decomposition (SVD).</p>

    <h4>Computed Homography Matrix</h4>
    <div class="matrix">
H = [<br>
&nbsp;&nbsp;[-0.0003222779,&nbsp;&nbsp;0.0000208189,&nbsp;&nbsp;0.9246967],<br>
&nbsp;&nbsp;[-0.0000956034,&nbsp;-0.0002599947,&nbsp;&nbsp;0.3807045],<br>
&nbsp;&nbsp;[-0.0000000377,&nbsp;&nbsp;0.0000000046,&nbsp;-0.0001140396]<br>
]
    </div>

    <h3>Second Image Pair</h3>
    <p>For the second pair of images, another set of six point pairs were manually selected:</p>
    <div class="image-pair">
        <img src="akkk.png" alt="Second image pair with point correspondences">
        <div class="image-caption">Second Image Pair - Point Correspondences Visualization</div>
    </div>

    <table class="point-table">
        <tr>
            <th>Point #</th>
            <th>Reference Image (x, y)</th>
            <th>Target Image (x', y')</th>
        </tr>
        <tr>
            <td>1</td>
            <td>(3519.66, 677.57)</td>
            <td>(550.07, 465.45)</td>
        </tr>
        <tr>
            <td>2</td>
            <td>(4183.82, 604.55)</td>
            <td>(1322.02, 531.52)</td>
        </tr>
        <tr>
            <td>3</td>
            <td>(3502.27, 2311.89)</td>
            <td>(480.52, 2322.32)</td>
        </tr>
        <tr>
            <td>4</td>
            <td>(4194.25, 2092.82)</td>
            <td>(1273.34, 2085.86)</td>
        </tr>
        <tr>
            <td>5</td>
            <td>(4489.82, 336.80)</td>
            <td>(1621.07, 368.09)</td>
        </tr>
        <tr>
            <td>6</td>
            <td>(4726.27, 302.02)</td>
            <td>(1840.14, 385.48)</td>
        </tr>
    </table>

    <h4>System Matrix A (Second Pair)</h4>
    <div class="matrix">
A = [<br>
&nbsp;&nbsp;[&nbsp;3519.65909,&nbsp;&nbsp;677.568182,&nbsp;&nbsp;1.00000,&nbsp;&nbsp;0.00000,&nbsp;&nbsp;0.00000,&nbsp;&nbsp;0.00000,&nbsp;-1936052.48,&nbsp;-372708.698,&nbsp;-550.068182],<br>
&nbsp;&nbsp;[&nbsp;0.00000,&nbsp;&nbsp;0.00000,&nbsp;&nbsp;0.00000,&nbsp;&nbsp;3519.65909,&nbsp;&nbsp;677.568182,&nbsp;&nbsp;1.00000,&nbsp;-1638241.32,&nbsp;-315377.190,&nbsp;-465.454545],<br>
&nbsp;&nbsp;[&nbsp;4183.81818,&nbsp;&nbsp;604.545455,&nbsp;&nbsp;1.00000,&nbsp;&nbsp;0.00000,&nbsp;&nbsp;0.00000,&nbsp;&nbsp;0.00000,&nbsp;-5531102.72,&nbsp;-799222.831,&nbsp;-1322.02273],<br>
&nbsp;&nbsp;[&nbsp;0.00000,&nbsp;&nbsp;0.00000,&nbsp;&nbsp;0.00000,&nbsp;&nbsp;4183.81818,&nbsp;&nbsp;604.545455,&nbsp;&nbsp;1.00000,&nbsp;-2223794.45,&nbsp;-321329.649,&nbsp;-531.522727],<br>
&nbsp;&nbsp;[&nbsp;3502.27273,&nbsp;&nbsp;2311.88636,&nbsp;&nbsp;1.00000,&nbsp;&nbsp;0.00000,&nbsp;&nbsp;0.00000,&nbsp;&nbsp;0.00000,&nbsp;-1682921.64,&nbsp;-1110913.94,&nbsp;-480.522727],<br>
&nbsp;&nbsp;[&nbsp;0.00000,&nbsp;&nbsp;0.00000,&nbsp;&nbsp;0.00000,&nbsp;&nbsp;3502.27273,&nbsp;&nbsp;2311.88636,&nbsp;&nbsp;1.00000,&nbsp;-8133391.63,&nbsp;-5368935.74,&nbsp;-2322.31818],<br>
&nbsp;&nbsp;[&nbsp;4194.25000,&nbsp;&nbsp;2092.81818,&nbsp;&nbsp;1.00000,&nbsp;&nbsp;0.00000,&nbsp;&nbsp;0.00000,&nbsp;&nbsp;0.00000,&nbsp;-5340710.11,&nbsp;-2664871.01,&nbsp;-1273.34091],<br>
&nbsp;&nbsp;[&nbsp;0.00000,&nbsp;&nbsp;0.00000,&nbsp;&nbsp;0.00000,&nbsp;&nbsp;4194.25000,&nbsp;&nbsp;2092.81818,&nbsp;&nbsp;1.00000,&nbsp;-8748633.56,&nbsp;-4365333.34,&nbsp;-2085.86364],<br>
&nbsp;&nbsp;[&nbsp;4489.81818,&nbsp;&nbsp;336.795455,&nbsp;&nbsp;1.00000,&nbsp;&nbsp;0.00000,&nbsp;&nbsp;0.00000,&nbsp;&nbsp;0.00000,&nbsp;-7278301.40,&nbsp;-545968.395,&nbsp;-1621.06818],<br>
&nbsp;&nbsp;[&nbsp;0.00000,&nbsp;&nbsp;0.00000,&nbsp;&nbsp;0.00000,&nbsp;&nbsp;4489.81818,&nbsp;&nbsp;336.795455,&nbsp;&nbsp;1.00000,&nbsp;-1652661.26,&nbsp;-123971.345,&nbsp;-368.090909],<br>
&nbsp;&nbsp;[&nbsp;4726.27273,&nbsp;&nbsp;302.022727,&nbsp;&nbsp;1.00000,&nbsp;&nbsp;0.00000,&nbsp;&nbsp;0.00000,&nbsp;&nbsp;0.00000,&nbsp;-8696986.31,&nbsp;-555763.003,&nbsp;-1840.13636],<br>
&nbsp;&nbsp;[&nbsp;0.00000,&nbsp;&nbsp;0.00000,&nbsp;&nbsp;0.00000,&nbsp;&nbsp;4726.27273,&nbsp;&nbsp;302.022727,&nbsp;&nbsp;1.00000,&nbsp;-1821870.72,&nbsp;-116422.897,&nbsp;-385.477273]<br>
]
    </div>

    <h4>Computed Homography Matrix (Second Pair)</h4>
    <div class="matrix">
H = [<br>
&nbsp;&nbsp;[-0.0003020561,&nbsp;&nbsp;0.0000070387,&nbsp;&nbsp;0.9354996043],<br>
&nbsp;&nbsp;[-0.0000801212,&nbsp;-0.0002550007,&nbsp;&nbsp;0.3533274960],<br>
&nbsp;&nbsp;[-0.0000000371,&nbsp;-0.0000000008,&nbsp;-0.0000907515]<br>
]
    </div>

    <h3>A.3: Image Warping</h3>
    <p>This section implements inverse warping with two from-scratch interpolation methods. Inverse warping ensures no holes appear in the output by sampling every output pixel location back through the inverse homography to find its corresponding source coordinates.</p>

    <h4>Implementation Overview</h4>
    <p>The warping pipeline consists of several key steps:</p>
    
    <div class="procedure">
        <div class="procedure-step"><strong>Step 1: Output Canvas Prediction</strong><br>
        Transform the four corners of the source image [0,0], [W-1,0], [W-1,H-1], [0,H-1] through the homography H to predict the output bounding box. Taking min/max of the transformed coordinates forms an integer bounding box that will contain the entire warped image.</div>
        
        <div class="procedure-step"><strong>Step 2: Inverse Mapping Grid</strong><br>
        Create a regular integer grid of output pixel centers (x,y). Each output location is back-projected using H⁻¹ to obtain continuous source coordinates (sx, sy). This inverse warping approach guarantees every output pixel gets a value.</div>
        
        <div class="procedure-step"><strong>Step 3: Interpolation</strong><br>
        For each back-projected coordinate, compute the pixel value using one of two interpolation methods (described below). An alpha mask tracks valid pixels to handle boundaries gracefully.</div>
    </div>

    <h4>Coordinate Convention</h4>
    <p>Throughout this implementation, we treat integer coordinates as pixel centers. Thus (0,0) represents the center of the first pixel, (0.5,0) lies between the first and second pixel, and so on. This convention simplifies bilinear interpolation mathematics and aligns with common computer vision practices.</p>

    <h4>Interpolation Methods</h4>
    
    <div class="method-comparison">
        <div class="method">
            <h4>Nearest Neighbor Interpolation</h4>
            <p>For each back-projected coordinate (sx, sy), round to the nearest integer pixel location (⌊sx⌉, ⌊sy⌉). If this location lies within the source image bounds, copy that pixel value and set alpha=1; otherwise leave the pixel as zero with alpha=0.</p>
            
            <p><strong>Advantages:</strong></p>
            <ul style="text-align: left; margin: 10px 0;">
                <li>Computationally efficient - simple rounding operations</li>
                <li>Preserves exact pixel values (no interpolation artifacts)</li>
                <li>Fastest execution time</li>
            </ul>
            
            <p><strong>Disadvantages:</strong></p>
            <ul style="text-align: left; margin: 10px 0;">
                <li>Produces blocky, aliased edges</li>
                <li>Visible pixel staircasing in diagonal lines</li>
                <li>Poor quality in high-frequency regions</li>
            </ul>
        </div>
        
        <div class="method">
            <h4>Bilinear Interpolation</h4>
            <p>For each back-projected coordinate (sx, sy), identify four neighboring pixels: (x₀, y₀), (x₁, y₀), (x₀, y₁), (x₁, y₁) where x₀ = ⌊sx⌋, x₁ = x₀+1, y₀ = ⌊sy⌋, y₁ = y₀+1. Compute fractional weights wx = sx - x₀ and wy = sy - y₀, then form the weighted sum:</p>
            
            <div class="equation" style="font-size: 0.9em; margin: 15px 0;">
                I = (1-wx)(1-wy)I₀₀ + wx(1-wy)I₁₀ + (1-wx)wy I₀₁ + wxwy I₁₁
            </div>
            
            <p>This is computed channel-wise for color images. Alpha is set to 1 only where all four neighbors are valid. Results are clipped to [0,255] and cast back to the input dtype.</p>
            
            <p><strong>Advantages:</strong></p>
            <ul style="text-align: left; margin: 10px 0;">
                <li>Smooth, visually pleasing results</li>
                <li>Reduces aliasing and jagged edges</li>
                <li>Better quality for most applications</li>
            </ul>
            
            <p><strong>Disadvantages:</strong></p>
            <ul style="text-align: left; margin: 10px 0;">
                <li>Increased computational cost</li>
                <li>Slight blurring compared to nearest neighbor</li>
                <li>More complex implementation</li>
            </ul>
        </div>
    </div>

    <h4>Warping Results Comparison</h4>
    <p>Both interpolation methods were applied to warp images using the computed homographies. The results demonstrate clear quality differences:</p>

    <div class="image-grid">
        <div class="image-pair">
            <img src="2.2.png" alt="Nearest Neighbor Warped">
            <div class="image-caption">Nearest Neighbor Result<br>Output: 13529×11109 pixels<br>Offset: (-10747, -3341)</div>
        </div>
        <div class="image-pair">
            <img src="3.2.png" alt="Bilinear Warped">
            <div class="image-caption">Bilinear Interpolation Result<br>Output: 13529×11109 pixels<br>Offset: (-10747, -3341)</div>
        </div>
    </div>

    <p><strong>Quality Analysis:</strong> Upon close inspection, the nearest neighbor result exhibits noticeable pixelation and stair-stepping artifacts, particularly visible along diagonal edges and in regions with high-frequency detail. The bilinear interpolation produces significantly smoother transitions and more natural-looking results, though at the cost of approximately 2-3x longer computation time. For most practical applications, the quality improvement of bilinear interpolation justifies the modest performance penalty.</p>

    <h4>Image Rectification</h4>
    <p>Rectification is a specific application of image warping where the goal is to transform a perspective view of a planar surface into a frontal-parallel (orthogonal) view. To validate the warping implementation, rectification was performed on images containing planar objects with known rectangular geometry. By manually selecting four corners of a planar surface and defining corresponding points in a rectangular target coordinate system, we can compute a rectifying homography that "undoes" the perspective distortion.</p>

    <h4>Rectification Methodology</h4>
    <p>For rectification, the user interactively selects four corners on a planar object (top-left, top-right, bottom-right, bottom-left). The algorithm computes the actual dimensions of the rectangle by measuring edge lengths:</p>
    
    <ul style="text-align: left; margin: 20px 40px;">
        <li><strong>Width:</strong> max(||p₁ - p₀||, ||p₂ - p₃||) where p₀, p₁ are top corners and p₂, p₃ are bottom corners</li>
        <li><strong>Height:</strong> max(||p₃ - p₀||, ||p₂ - p₁||) where left and right edges are measured</li>
    </ul>
    
    <p>This preserves the aspect ratio of the original object. The destination points are then defined as corners of a rectangle with these dimensions: [(0,0), (width-1,0), (width-1,height-1), (0,height-1)]. Computing the homography between source and destination points yields a rectifying transformation.</p>

    <h4>Rectification Examples</h4>
    <p>Two images were selected for rectification demonstrating the technique on different planar surfaces:</p>

    <h4>Rectification Example 1</h4>
    <div class="image-grid">
        <div class="image-pair">
            <img src="4.5.png" alt="Rectified Example 1 Bilinear">
            <div class="image-caption">Bilinear</div>
        </div>
        <div class="image-pair">
            <img src="4.5.png" alt="Rectified Example 1 Nearest Neighbor">
            <div class="image-caption">Nearest Neighbor</div>
        </div>
    </div>

    <h4>Rectification Example 2</h4>
    <div class="image-grid">
        <div class="image-pair">
            <img src="4.4.png" alt="Rectified Example 2 Bilinear">
            <div class="image-caption">Bilinear</div>
        </div>
        <div class="image-pair">
            <img src="4.4.png" alt="Rectified Example 2 Nearest Neighbor">
            <div class="image-caption">Nearest Neighbor</div>
        </div>
    </div>

    <p><strong>Validation:</strong> Successful rectification confirms that both the homography computation and warping functions work correctly. Known geometric properties (e.g., parallel lines remain parallel, right angles are preserved) are restored in the rectified output, demonstrating the mathematical correctness of the implementation.</p>

    <h4>Alpha Mask and Boundary Handling</h4>
    <p>Both warping functions return an alpha mask alongside the warped image. The alpha channel is binary (0 or 1) and indicates which pixels contain valid data versus empty regions outside the source image bounds. This alpha mask is crucial for:</p>
    
    <ul style="text-align: left; margin: 20px 40px;">
        <li>Visualizing the coverage area of the warped image</li>
        <li>Properly blending multiple images in the mosaicing stage</li>
        <li>Identifying and cropping excess black borders</li>
        <li>Computing statistics like valid-pixel fraction</li>
    </ul>

    <p>The valid-pixel fraction typically ranges from 0.4-0.7 depending on the degree of rotation and the aspect ratio changes induced by the homography. Bilinear interpolation may have slightly fewer valid pixels near boundaries since it requires all four neighbors to be in-bounds, whereas nearest neighbor only needs one.</p>

    <h3>A.4: Image Blending and Mosaicing</h3>
    <p>The final stage combines multiple warped images into seamless panoramic mosaics. This process uses alpha blending with linear falloff masks to minimize visible seams and edge artifacts in overlapping regions. The implementation follows a one-shot warping approach where one image remains in its original coordinate system (the reference) while other images are warped into its projection.</p>

    <h4>Blending Strategy</h4>
    <p>Rather than simply overlaying images (which creates harsh boundaries), we use weighted averaging based on alpha masks. Each pixel's final value is computed as a weighted sum of contributions from all images that cover that location, where weights are determined by the alpha masks.</p>

    <h4>Mosaicing Procedure</h4>
    
    <div class="procedure">
        <div class="procedure-step"><strong>Step 1: Canvas Size Determination</strong><br>
        Calculate the bounding box that encompasses all images after warping. The reference image (img2) occupies coordinates [0, width] × [0, height], while the warped image (img1) has an offset determined by its homography transformation. The canvas dimensions are computed as:<br>
        <span style="font-family: 'Courier New', monospace; display: block; margin-top: 8px;">
        x_min = min(0, x_offset_warped)<br>
        x_max = max(width_ref, x_offset_warped + width_warped)<br>
        y_min = min(0, y_offset_warped)<br>
        y_max = max(height_ref, y_offset_warped + height_warped)
        </span>
        This ensures the entire mosaic fits on the canvas without cropping any image content.</div>
        
        <div class="procedure-step"><strong>Step 2: Alpha Mask Generation</strong><br>
        Create smooth alpha masks with linear falloff from center to edges for each image. The mask is computed using:<br>
        <span style="font-family: 'Courier New', monospace; display: block; margin-top: 8px;">
        alpha(x, y) = (1 - |x_normalized|) × (1 - |y_normalized|)
        </span>
        where x_normalized and y_normalized range from -1 (at edges) to 0 (at center). This produces alpha=1 at the image center, smoothly falling to 0 at boundaries. The linear falloff ensures gradual transitions in overlapping regions.</div>
        
        <div class="procedure-step"><strong>Step 3: Image Placement on Canvas</strong><br>
        Position each image on the canvas according to its coordinate system:
        <ul style="margin: 10px 0; padding-left: 20px;">
            <li><strong>Reference image (img2):</strong> Placed at offset (0 - x_min, 0 - y_min) to account for canvas origin shift</li>
            <li><strong>Warped image (img1):</strong> Placed at offset (x_offset - x_min, y_offset - y_min)</li>
        </ul>
        Each image is multiplied by its alpha mask and accumulated into a weighted sum canvas.</div>
        
        <div class="procedure-step"><strong>Step 4: Black Pixel Handling</strong><br>
        Invalid pixels (black regions outside the original image bounds) are excluded from blending. A validity mask is created by checking if pixels have non-zero values:<br>
        <span style="font-family: 'Courier New', monospace; display: block; margin-top: 8px;">
        valid_mask = (image > 0)  # for grayscale<br>
        valid_mask = any(image > 0, axis=channel)  # for color
        </span>
        The final alpha is computed as alpha_mask × valid_mask, ensuring black pixels contribute zero weight.</div>
        
        <div class="procedure-step"><strong>Step 5: Weighted Averaging Blending</strong><br>
        For each pixel location (x, y) on the canvas, compute the blended value as:<br>
        <span style="font-family: 'Courier New', monospace; display: block; margin-top: 8px;">
        mosaic(x,y) = (Σ image_i(x,y) × alpha_i(x,y)) / (Σ alpha_i(x,y))
        </span>
        where the sum is over all images covering that pixel. This weighted average produces smooth transitions in overlap regions. A small epsilon (1e-10) is added to the denominator to avoid division by zero.</div>
        
        <div class="procedure-step"><strong>Step 6: Boundary Cropping</strong><br>
        Remove empty black borders from the final mosaic by detecting the bounding box of non-zero regions:
        <span style="font-family: 'Courier New', monospace; display: block; margin-top: 8px;">
        mask = (mosaic > 0)<br>
        rows_with_content = any(mask, axis=columns)<br>
        cols_with_content = any(mask, axis=rows)<br>
        crop to [row_min:row_max+1, col_min:col_max+1]
        </span>
        This produces a tightly cropped final mosaic without wasted canvas space.</div>
    </div>

    <h4>Technical Implementation Details</h4>
    <p><strong>Coordinate System:</strong> The implementation uses a "fold into reference" approach where image2 remains unwarped (identity transformation) and image1 is warped into image2's coordinate system via the homography H. This simplifies the blending logic since one image maintains its original pixel grid.</p>
    
    <p><strong>Memory Efficiency:</strong> All blending operations use float32 intermediate arrays to accumulate weighted sums precisely, then convert back to uint8 after normalization and clipping to [0, 255].</p>
    
    <p><strong>Edge Artifact Reduction:</strong> The linear alpha falloff combined with weighted averaging significantly reduces visible seams. While more sophisticated techniques like Laplacian pyramid blending could further minimize high-frequency ghosting, simple feathering proves sufficient for mosaics with good overlap and consistent lighting.</p>

    <h4>Mosaic Results</h4>
    <p>Three distinct panoramic mosaics were created from pairs of overlapping images. Each mosaic demonstrates successful registration, smooth blending, and minimal visible seams:</p>

    <div style="display: flex; flex-direction: column; gap: 40px; margin: 30px 0;">
        <div style="text-align: center;">
            <img src="2.1.png" alt="Mosaic 1" style="max-width: 100%; height: auto; border: 1px solid #ddd; box-shadow: 0 2px 5px rgba(0,0,0,0.1);">
            <div class="image-caption">Mosaic 1: Urban Scene<br>Seamless blending with weighted averaging</div>
        </div>
        
        <div style="text-align: center;">
            <img src="3.3.png" alt="Mosaic 2" style="max-width: 100%; height: auto; border: 1px solid #ddd; box-shadow: 0 2px 5px rgba(0,0,0,0.1);">
            <div class="image-caption">Mosaic 2: Architectural Scene<br>Wide-angle panoramic capture</div>
        </div>
        
        <div style="text-align: center;">
            <img src="3.5.png" alt="Mosaic 3" style="max-width: 100%; height: auto; border: 1px solid #ddd; box-shadow: 0 2px 5px rgba(0,0,0,0.1);">
            <div class="image-caption">Mosaic 3: Natural Landscape<br>Smooth transitions in outdoor scene</div>
        </div>
    </div>

    <h2>Part B: Feature Matching and Autostitching</h2>
    
    <div class="intro">
        <p>Part B extends the manual mosaicing pipeline with automatic feature detection and matching. Instead of manually selecting correspondence points, this section implements Harris corner detection, Adaptive Non-Maximal Suppression (ANMS), feature descriptor extraction, feature matching with Lowe's ratio test, and RANSAC-based robust homography estimation. The result is a fully automated image stitching system that produces panoramas comparable in quality to manual methods.</p>
    </div>

    <h3>B.1: Harris Corner Detection and Adaptive Non-Maximal Suppression</h3>
    
    <h4>Harris Corner Detection</h4>
    <p>I begin by detecting interest points using a single-scale Harris corner detector. Starting from a grayscale image normalized to [0,1], I compute image gradients I<sub>x</sub> and I<sub>y</sub> using Sobel operators. From these gradients, I form the second-moment matrix components I<sub>x</sub>², I<sub>y</sub>², and I<sub>x</sub>I<sub>y</sub>. Each of these components is then smoothed with a Gaussian filter (σ = 1.5) to produce S<sub>xx</sub>, S<sub>yy</sub>, and S<sub>xy</sub>.</p>

    <p>The Harris corner response is computed as:</p>
    <div class="equation">
        R = det(M) - κ(trace(M))²
    </div>
    <p>where M is the structure tensor matrix [S<sub>xx</sub>, S<sub>xy</sub>; S<sub>xy</sub>, S<sub>yy</sub>] and κ = 0.04. I normalize R to [0,1] and apply 3×3 non-maximum suppression, keeping only pixels above a quantile threshold (default 0.995) that are local maxima. This initial detection typically yields thousands of corner candidates.</p>

    <h4>Harris Corner Results</h4>
    <p>The Harris detector produces thousands of corner points across the image, often clustered in regions with rich texture. Below shows the raw Harris corners overlaid on the original image before any spatial filtering:</p>

    <div class="image-pair" style="margin: 30px 0;">
        <img src="harris_anms_comparison.png" alt="Harris corners before ANMS">
        <div class="image-caption">Harris Corner Detection Results<br>Dense corner detections before ANMS - note the clustering in textured regions</div>
    </div>

    <h4>Adaptive Non-Maximal Suppression (ANMS)</h4>
    <p>The dense Harris corners are not ideal for matching because they cluster heavily in high-texture areas, providing poor spatial coverage. ANMS addresses this by selecting a well-distributed subset based on suppression radii. For each candidate corner i with Harris response r<sub>i</sub>, I compute its suppression radius:</p>
    
    <div class="equation">
        r<sub>i</sub>* = min<sub>j</sub> ||x<sub>i</sub> - x<sub>j</sub>||
    </div>
    
    <p>where the minimum is taken over all corners j satisfying r<sub>j</sub> > c·r<sub>i</sub> with robustness constant c = 0.9. If no such stronger corners exist nearby, I fall back to using the nearest neighbor distance. I then select the 1000 corners with the largest suppression radii, ensuring both spatial distribution and strong corner responses.</p>

    <h4>ANMS Results</h4>
    <p>ANMS successfully transforms the dense, clustered detections into a spatially balanced feature set. Below shows the well-distributed corners after ANMS filtering:</p>

    <div class="image-pair" style="margin: 30px 0;">
        <img src="harris_anms_comparison.png" alt="ANMS selected corners">
        <div class="image-caption">ANMS Selected Corners<br>1000 spatially distributed features selected from thousands of detections</div>
    </div>

    <h3>B.2: Feature Descriptor Extraction</h3>
    
    <h4>Implementation</h4>
    <p>For each keypoint selected by ANMS, I extract an 8×8 feature descriptor that captures the local appearance around that point. The extraction process begins by creating a blurred base image: I convert to grayscale in [0,1] and apply a Gaussian blur with σ = 2.0 using a separable filter for efficiency.</p>

    <p>Around each keypoint (x, y), I extract a 40×40 pixel window using bilinear interpolation to handle sub-pixel coordinates accurately. This larger window is crucial - it provides the "blurred" descriptor mentioned in the paper that offers robustness to small localization errors. I discard keypoints within 20 pixels of the image border to ensure the full 40×40 window stays in-bounds.</p>

    <p>Next, I downsample the 40×40 window to an 8×8 descriptor grid. I achieve this by dividing the window into 5×5 non-overlapping cells and computing the average intensity within each cell (average pooling). This gives me a compact 8×8 patch representation.</p>

    <p>The final critical step is bias/gain normalization for photometric invariance. I flatten the 8×8 patch into a 64-dimensional vector, then normalize it to have zero mean and unit standard deviation by subtracting the mean and dividing by the standard deviation (with a small epsilon for numerical stability). This normalization makes the descriptor invariant to linear brightness and contrast changes.</p>

    <h4>Feature Descriptor Visualization</h4>
    <p>Below shows several extracted 8×8 feature descriptors sampled from different keypoint locations:</p>

    <div class="image-pair" style="margin: 30px 0;">
        <img src="feature_descriptors_grid.png" alt="Feature descriptors">
        <div class="image-caption">Sample 8×8 Feature Descriptors<br>Each patch is extracted from a 40×40 window and bias/gain normalized<br>Descriptors are zero-mean, unit-variance for photometric invariance</div>
    </div>

    <h3>B.3: Feature Matching</h3>
    
    <h4>Matching Implementation</h4>
    <p>I match the B.2 descriptors by computing all pairwise squared Euclidean distances between the zero-mean/unit-std 64-dimensional vectors from the two images. For computational efficiency, I use the expanded distance formula:</p>
    
    <div class="equation">
        ||a - b||² = ||a||² + ||b||² - 2a<sup>T</sup>b
    </div>
    
    <p>This allows me to precompute the squared norms and perform fast matrix multiplication for the cross terms, avoiding explicit loops over descriptor pairs.</p>

    <p>For each descriptor in image 1, I find the nearest (d₁) and second-nearest (d₂) neighbors in image 2 using a fast partial sort. I then apply Lowe's ratio test d₁/d₂ < τ with a tunable threshold (default 0.8, following Figure 6b in the paper). This threshold filters out ambiguous matches where multiple similar features exist. I add a small epsilon to guard against small denominators in the ratio computation.</p>

    <p>I optionally enforce mutual (cross-check) consistency by requiring that image 2's nearest neighbor of the chosen match also points back to the same image 1 feature. This bidirectional verification significantly reduces false matches.</p>

    <p>For robustness, I trim both descriptor and keypoint lists to their common minimum length to keep indices properly aligned. The accepted matches are saved to a B3_matches.csv file with columns (i₁, x₁, y₁, i₂, x₂, y₂, dist, ratio) recording the feature indices, coordinates, distance, and ratio value for each correspondence. For visualization, I stack the two images side-by-side horizontally, plot the matched keypoints as circles, and draw connecting lines between correspondences (capped at 300 matches for visual clarity).</p>

    <h4>Feature Matching Results</h4>
    <p>After applying Lowe's ratio test, I obtain high-confidence feature correspondences. Below shows the matched features with connecting lines for three different image pairs:</p>

    <div class="image-grid">
        <div class="image-pair">
            <img src="feature_matches.png" alt="Feature matches - first pair">
            <div class="image-caption">Feature Matching: First Image Pair<br>Matched keypoints after ratio test (threshold = 0.8)</div>
        </div>
        <div class="image-pair">
            <img src="mosaic2_matches.png" alt="Feature matches - second pair">
            <div class="image-caption">Feature Matching: Second Image Pair<br>Correspondences established through descriptor similarity</div>
        </div>
        <div class="image-pair">
            <img src="mosaic3_matches.png" alt="Feature matches - third pair">
            <div class="image-caption">Feature Matching: Third Image Pair<br>High-confidence matches with connecting lines</div>
        </div>
    </div>

    <h3>B.4: RANSAC for Robust Homography and Automatic Mosaicing</h3>
    
    <h4>4-Point RANSAC Implementation</h4>
    <p>While feature matching produces many correspondences, outliers are inevitable due to repetitive patterns, occlusions, and descriptor ambiguity. I implement 4-point RANSAC from scratch to robustly estimate the homography despite these outliers.</p>

    <p>In each RANSAC iteration, I randomly sample 4 correspondence pairs from the B.3 matches (P<sub>i</sub> ↔ Q<sub>i</sub>). Four points is the minimum required to solve for a homography. I skip degenerate configurations where the 4 points form quads with near-zero area. For each valid sample, I fit a homography H using the Direct Linear Transform (DLT) method. I tried both normalized DLT and the Ah=b formulation, keeping whichever performs best.</p>

    <p>To score each candidate homography, I compute the reprojection error for all N correspondences. By default, I use the symmetric transfer error:</p>
    
    <div class="equation">
        error = ||HP - Q||² + ||H<sup>-1</sup>Q - P||²
    </div>
    
    <p>This bidirectional error is more robust than one-sided reprojection. A correspondence is classified as an inlier if its error is below 3 pixels. I track the homography with the largest inlier count across iterations (default 3000 iterations maximum).</p>

    <p>The implementation exposes several tunable parameters: --iters (iteration count), --thresh (inlier threshold in pixels), --method (normalized DLT or Ah=b), and --one_sided (use ||HP - Q|| only instead of symmetric error). These allow testing different configurations.</p>

    <p>Once RANSAC converges to the best model, I re-estimate H using all inlier points rather than just the 4-point subset. This least-squares refinement on the consensus set produces a more accurate final homography. The implementation logs inlier statistics and saves per-pair visualizations overlaying green (inlier) vs. red (outlier) matches.</p>

    <h4>Automatic Mosaicing</h4>
    <p>Using the RANSAC-estimated homography, I produce automatic panoramic mosaics by adapting the warping and blending pipeline from Part A. For mosaicing, I choose one image as the reference and set H<sub>ref</sub> = I (identity transform). Each non-reference image gets its own homography H that aligns it to this reference frame.</p>

    <p>I compute the global canvas bounds by warping all image corners through their respective homographies, then applying translation offsets to ensure all content fits. Each image is inverse-warped to the canvas using bilinear interpolation by default (nearest-neighbor interpolation is available as an option via --nn flag).</p>

    <p>For blending, I use feathered alpha averaging where alpha values smoothly fall off toward image edges. This creates seamless transitions in overlapping regions. I accumulate weighted intensities (canvas += image × alpha) and normalize by the accumulated weights to produce the final blended mosaic. Empty black borders are automatically cropped by detecting the bounding box of non-zero content.</p>

    <h4>Comparison: Manual vs Automatic Stitching</h4>
    <p>Below I compare mosaics created manually (Part A with hand-selected correspondences) versus automatically (Part B with RANSAC). The same image pairs are used for both methods. The warping techniques differ slightly because I updated the code, but you can see that the middle area now has much less blurriness. This improvement comes from the automatic alignment being.</p>

    <div style="margin: 40px 0;">
        <h4>Mosaic Set 1</h4>
        <div class="image-grid">
            <div class="image-pair">
                <img src="apart_mosaic1.png" alt="Manual mosaic 1">
                <div class="image-caption">Manual Stitching (Part A)<br>Hand-selected correspondence points</div>
            </div>
            <div class="image-pair">
                <img src="mosaic1.png" alt="Automatic mosaic 1">
                <div class="image-caption">Automatic Stitching (Part B)<br>RANSAC with detected features<br>Notice the reduced blurriness in overlapping regions</div>
            </div>
        </div>
    </div>

    <div style="margin: 40px 0;">
        <h4>Mosaic Set 2</h4>
        <div class="image-grid">
            <div class="image-pair">
                <img src="apart_mosaic2.png" alt="Manual mosaic 2">
                <div class="image-caption">Manual Stitching (Part A)<br>Hand-selected correspondence points</div>
            </div>
            <div class="image-pair">
                <img src="mosaic2.png" alt="Automatic mosaic 2">
                <div class="image-caption">Automatic Stitching (Part B)<br>60-100 RANSAC inliers for accurate alignment<br>Sharper blending from better homography estimation</div>
            </div>
        </div>
    </div>

    <div style="margin: 40px 0;">
        <h4>Mosaic Set 3</h4>
        <div class="image-grid">
            <div class="image-pair">
                <img src="apart_mosaic3.png" alt="Manual mosaic 3">
                <div class="image-caption">Manual Stitching (Part A)<br>Hand-selected correspondence points</div>
            </div>
            <div class="image-pair">
                <img src="Image_20251017192113.jpg" alt="Automatic mosaic 3">
                <div class="image-caption">Automatic Stitching (Part B)<br>Robust and consistent results<br>Improved alignment quality visible in seam regions</div>
            </div>
        </div>
    </div>

    <h4>Analysis</h4>
    <p>The automatic pipeline demonstrates clear advantages over manual stitching:</p>
    <ul>
        <li><strong>Accuracy:</strong> Using 60-100 inlier points produces more accurate homographies than manual selection of just a few points. The geometric relationship is better captured, especially at image boundaries.</li>
        <li><strong>Speed:</strong> Automatic processing completes in seconds versus several minutes of manual point selection and verification.</li>
        <li><strong>Consistency:</strong> The automated approach eliminates human variability - running the pipeline multiple times yields identical results.</li>
        <li><strong>Robustness:</strong> RANSAC handles 20-40% outlier rates effectively, reliably recovering correct geometry across diverse scenes, lighting conditions, and texture patterns.</li>
        <li><strong>Scalability:</strong> The automatic pipeline easily extends to processing many image pairs without proportional increases in human effort.</li>
    </ul>

    <div class="highlight-box">
        <strong>What I Learned:</strong> The most valuable insight from this project was understanding how computer vision algorithms compose into robust end-to-end systems. Each component - Harris detection, ANMS, descriptor matching, RANSAC - has limitations individually, but their integration creates a remarkably reliable pipeline. RANSAC's ability to find correct geometry despite 25-40% outliers was particularly striking, demonstrating the power of robust estimation. The automatic approach not only matches manual quality but often exceeds it by using more correspondences and avoiding selection bias. Success in practical computer vision comes from thoughtful algorithm integration rather than relying on any single technique.
    </div>

    <h2>Implementation Details</h2>
    
    <h3>Technical Approach</h3>
    <p>The implementation follows a systematic pipeline: point correspondence selection, homography computation via SVD, inverse warping with two interpolation methods, and alpha blending with linear masks. All components were implemented from scratch without using high-level computer vision libraries.</p>

    <h3>Challenges and Solutions</h3>
    <ul>
        <li><strong>Large Image Handling:</strong> Implemented efficient coordinate transformation using meshgrid operations</li>
        <li><strong>Numerical Stability:</strong> Used SVD for robust homography computation with overdetermined systems</li>
        <li><strong>Seam Reduction:</strong> Developed alpha masks with linear falloff for smooth blending</li>
        <li><strong>Memory Management:</strong> Optimized warping algorithms to handle high-resolution images</li>
    </ul>

    <h2>Conclusion</h2>
    <p>This project successfully demonstrates the complete image mosaicing pipeline from image capture to final composite. The implementation achieves sub-pixel accuracy in homography computation and produces visually seamless mosaics through careful warping and blending. The comparison between interpolation methods shows the trade-offs between computational efficiency and visual quality, with bilinear interpolation providing superior results for most applications.</p>

    <p>The techniques developed here form the foundation for more advanced computer vision applications, including panoramic photography, augmented reality, and 3D reconstruction from multiple views.</p>

</body>
</html>
